---
title: "HW4"
output: html_document
date: '2022-05-03'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
library(tidyverse)
library(tidymodels)
library(ggplot2)
library(visdat)
library(corrplot)
library(discrim)
library(klaR)
library(yardstick)
```

```{r}
set.seed(231)
data = read.csv('titanic.csv')
data$survived = factor(data$survived, levels = c('Yes', 'No'))
data$pclass = factor(data$pclass)
```


## Question 1
```{r}
data_split <- initial_split(data, prop = 0.70,
                                strata = survived)
titanic_train <- training(data_split)
titanic_test <- testing(data_split)
```

```{r}
dim(titanic_train)
dim(titanic_test)
```


## Question 2
```{r}
cv_folds <- vfold_cv(titanic_train, v = 10)
cv_folds
```


## Question 3
 K-fold cross-validation is a method to find the best parameter that yields the "closest" fit of the model. It holds out a subset of the training observations from the fitting process, and apply the learned model to those held out observations.
 We should use it rather than simply fitting and testing models on the entire training set. Because rather than fitting and testing models on the entire training set, fitting and testing models on the part of training set(ex- one set of 10 folds) usually have lower RMSE(it means better fitting). 
   If we did use the entire training set, Bootstrap would be used as the resampling method of this case. This method does not divide train set.


## Question 4
```{r}
titanic_recipe = recipe(survived ~ pclass + sex + age + sib_sp + parch + fare, 
                        data = titanic_train) %>%
        step_impute_linear(age) %>%
        step_dummy(all_nominal_predictors()) %>%
        step_interact(terms = ~ starts_with("sex"):fare + age:fare)

# logistic regression with the glm enigine
log_reg = logistic_reg() %>% 
        set_engine("glm") %>% 
        set_mode("classification")

log_wkflow = workflow() %>% 
        add_model(log_reg) %>% 
        add_recipe(titanic_recipe)

log_fit = fit(log_wkflow, titanic_train)

# linear discriminant analysis with the MASS engine
lda_mod = discrim_linear() %>%
        set_engine("MASS") %>%
        set_mode("classification")

lda_wkflow = workflow() %>% 
        add_model(lda_mod) %>% 
        add_recipe(titanic_recipe)

lda_fit = fit(lda_wkflow, titanic_train)

# Quadratic discriminant analysis with the MASS engine
qda_mod = discrim_quad() %>% 
        set_mode("classification") %>% 
        set_engine("MASS")

qda_wkflow = workflow() %>% 
        add_model(qda_mod) %>% 
        add_recipe(titanic_recipe)

qda_fit = fit(qda_wkflow, titanic_train)
```
There are 3 models. There are 10 folds for each model. So, in total, I will be fitting 30 models to the data. 


## Question 5
```{r eval=FALSE}
log_res = log_wkflow %>% 
        fit_resamples(resamples = cv_folds, 
                      metrics = metric_set(recall, precision, 
                                           accuracy, sens, spec, roc_auc),
                      control = control_resamples(save_pred = TRUE)) 

lda_res = lda_wkflow %>%
        fit_resamples(resamples = cv_folds,
                      metrics = metric_set(recall, precision, 
                                           accuracy, sens, spec, roc_auc),
                      control = control_resamples(save_pred = TRUE))

qda_res = qda_wkflow %>%
        fit_resamples(resamples = cv_folds,
                      metrics = metric_set(recall, precision, 
                                           accuracy, sens, spec, roc_auc),
                      control = control_resamples(save_pred = TRUE))
```


## Question 6
```{r}
### collect_metrics(log_res) 
### I wrote the code like this because we can't run the code because of the Question 5's 'eval=FALSE' code. 
```
```{r}
### collect_metrics(lda_res)
### I wrote the code like this because we can't run the code because of the Question 5's 'eval=FALSE' code. 
```
```{r}
### collect_metrics(qda_res)
### I wrote the code like this because we can't run the code because of the Question 5's 'eval=FALSE' code. 
```
   The logistic model performed the best. The mean accuracy is 0.80 and standard error is 0.01. Because the accuracy of logistic model is the highest one, and the standard error of logistic model is the smallest one. So, it has perfomed the best. 


## Question 7
```{r}
log_fit = fit(log_wkflow, titanic_train)
```


## Question 8
```{r}
p1 <- predict(log_fit, new_data = titanic_test, type = "prob")

pred1 <- bind_cols(p1, titanic_test)

```
```{r}
log_acc = augment(log_fit, new_data = titanic_test) %>%
  accuracy(truth = survived, estimate = .pred_class)

log_acc
```
My models' testing accuracy is 0.82. Its average accuracy across folds was 0.80. 

## Question 9
$Q = \sum_{i=1}^n (y_i-\hat{y_i})^2$
$\hat{y_i}=\beta$
$Q = \sum_{i=1}^n (y_i-\beta)^2$
$\  \frac{d Q}{d \beta} = -2\sum_{i=1}^n (y_i-\beta) = 0$
$\  \sum_{i=1}^n y_i - n\beta = 0$
$\  n\beta = \sum_{i=1}^n y_i$
$\  \hat{\beta} = \frac{\sum_{i=1}^n y_i}{n} = \bar{y}$

## Question 10
In LOOCV, the covariance between β^(1), and β^(2) would be greater than the k-fold cross-validation's one.




